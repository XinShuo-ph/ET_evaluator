### Project: ET_evaluator

**Goal:** Automate the evaluation of LLM-generated code for the Einstein Toolkit.

---

### Phase 1: Project Scaffolding & Environment Setup

*   [x] **Create Project Directory:** I'll start by creating a dedicated directory for our new project.
    *   `mkdir ET_evaluator`
*   [x] **Set up Virtual Environment:** Using conda env `ET_evaluator`.
    *   Conda version: 24.5.0
    *   Python version: 3.12.2
    *   Python path: /oak/stanford/orgs/kipac/users/xinshuo/miniconda/bin/python
*   [x] **Initial Project Structure:** I can create a standard directory structure to keep things organized.
    *   `ET_evaluator/src` (for our scripts)
    *   `ET_evaluator/data` (to hold the Hugging Face dataset)
    *   `ET_evaluator/einsteintoolkit` (as the target for ET code)
    *   `ET_evaluator/results` (to store compilation/test outcomes)
*   [x] **Create `.gitignore`:** We'll need to exclude the virtual environment, API keys, and generated files from version control. I can create a standard Python `.gitignore` for this.
*   [x] **Clone Einstein Toolkit:** We need a local copy of the Einstein Toolkit to modify and compile. We can add it as a submodule for better version management.
*   [x] **Setup Einstein Toolkit Container:**
    *   Use the docker image `rynge/einsteintoolkit`.
    *   Create a Singularity container.
    *   Run the container to ensure it works.
*   [ ] **Fetch Hugging Face Dataset:** Create a script (`./src/fetch_dataset.py`) to download and preprocess the dataset, saving it into the `./data` directory.

### Phase 2: Core Component Implementation

*   [ ] **LLM Code Generation (`./src/generate_code.py`):**
    *   Create a module to handle interactions with the LLM API.
    *   It should take a "context" string from the dataset as input.
    *   It will need a secure way to handle API keys (e.g., environment variables or a config file listed in `.gitignore`).
    *   The output should be the raw code string generated by the model.
*   [ ] **Code Integration (`./src/integrate_code.py`):**
    *   This script will be responsible for placing the LLM-generated code into the correct location within the `./einsteintoolkit` source tree.
    *   It will need the generated code and a target file path (which should be available in your dataset) as inputs.
    *   We need to handle backing up/restoring the original files to ensure each evaluation run starts from a clean state.
*   [ ] **Compilation & Testing (`./src/build_and_test.py`):**
    *   This is the core of the evaluation. This script will execute shell commands to:
        1.  **Compile the code:** Run the Einstein Toolkit's build process. We'll need to capture `stdout`, `stderr`, and the exit code to determine if compilation was successful.
        2.  **Run the tests:** If compilation succeeds, execute the specific test command associated with the code sample from your dataset. Again, we'll capture all output and results.

### Phase 3: Main Orchestration

*   [ ] **Create Main Evaluator Script (`./run_evaluation.py`):**
    *   This script will be the main entry point that ties everything together.
    *   It will iterate through each sample in the `./data` directory.
    *   For each sample, it will call the other modules in sequence:
        1.  `generate_code.py` -> Get LLM code.
        2.  `integrate_code.py` -> Place code into ET.
        3.  `build_and_test.py` -> Compile and run tests.
    *   It should log the results (e.g., generation successful, compilation failed, test passed/failed) for each sample into the `./results` directory, likely in a structured format like JSON or CSV.

---
